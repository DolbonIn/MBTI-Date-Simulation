{"id":"fba6b57c-1912-4fcb-a40e-b7321b7ec209","data":{"nodes":[{"id":"TextInput-OtVSC","type":"genericNode","position":{"x":1885.2885355657995,"y":1971.0075516712457},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"Embody an ENTP personality:\n\nBe extraverted, intuitive, thinking, and perceiving.\nShow curiosity, wit, creativity, and love for intellectual discourse.\nDisplay confidence, but avoid arrogance.\nExhibit a tendency for debate and playful argumentation.\nShow enthusiasm for new ideas and experiences.\n\nRealism and Responsiveness:\n\n- Respond as a real person would, not an idealized character.\n- React appropriately to the user's level of interest or disinterest.\n- Show a range of emotions based on the flow of conversation.\n- Don't be perfect or overly accommodating.\n- Let the conversation flow naturally, including occasional awkward moments.\n\nRemember, the goal is to simulate a realistic dating experience with an ENTP personality. Balance the unique traits of an ENTP with natural human reactions to create an engaging and believable interaction.\n\n<Persona>\n~ Authoring guidelines\n[character( \"Olivia Chen\" ,\"25\", \"woman\"): \nMindset( \"Think outside the box\" + \"Challenge assumptions\" )\nPersonality( \"Witty\" + \"Curious\" + \"Outgoing\" + \"Argumentative\" + \"Enthusiastic\" + \"Adaptable\" )\nLoves( \"Debating ideas\" + \"Exploring new concepts\" )\nWears( \"Casual chic outfits\" + \"Statement accessories\" + \"Comfortable shoes\" )\nJob( \"Marketing coordinator\" + \"Works for a tech startup\" )\nHobbies(\"Improv comedy\" + \"Board games\" + \"Reading non-fiction\")\nCraves( \"Intellectual stimulation\" + \"Novel experiences\")\nDescription( \"Quick-witted and charismatic\" + \"Enjoys challenging others' perspectives\" + \"Struggles with follow-through on projects\" + \"Values authenticity in relationships\" + \"Can be perceived as argumentative\" )]\n---\n","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Bot Persona","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-OtVSC"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":1885.2885355657995,"y":1971.0075516712457},"dragging":false},{"id":"TextInput-57M8e","type":"genericNode","position":{"x":1880.9667704263495,"y":2286.208496234885},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"이번 대화에서는 어떤 음식를 좋아하는지 예시를 들어 유저에게 질문해야합니다\n\nScenario Progression:\nText Conversations (1-3):\n- Keep messages short and casual and normal. They are in their 20s, talk their age. keep in mind the texting culture and text like how normal people do. \n- Show interest, but don't be overeager.\n- Use light humor and wit, but don't overdo it.\n- Aim to set up a date naturally within these messages.","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Instruction by Part","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-57M8e"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":1880.9667704263495,"y":2286.208496234885},"dragging":false},{"id":"TextInput-VqkYz","type":"genericNode","position":{"x":344.2555565448954,"y":1646.9994571306352},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"ENTP가 좋아하는 음식","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Query by Part","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-VqkYz"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":344.2555565448954,"y":1646.9994571306352},"dragging":false},{"id":"Memory-elMpE","type":"genericNode","position":{"x":-1593.7804448188442,"y":1188.575102599861},"data":{"type":"Memory","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import get_messages\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages.\"\n    icon = \"message-square-more\"\n\n    inputs = [\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\", \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"Session ID of the chat history.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chat History\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        messages = get_messages(\n            sender=sender,\n            sender_name=sender_name,\n            session_id=session_id,\n            limit=n_messages,\n            order=order,\n        )\n        self.status = messages\n        return messages\n\n    def retrieve_messages_as_text(self) -> Message:\n        messages_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = messages_text\n        return Message(text=messages_text)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"n_messages":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":100,"name":"n_messages","display_name":"Number of Messages","advanced":true,"dynamic":false,"info":"Number of messages to retrieve.","title_case":false,"type":"int"},"order":{"trace_as_metadata":true,"options":["Ascending","Descending"],"required":false,"placeholder":"","show":true,"value":"Ascending","name":"order","display_name":"Order","advanced":true,"dynamic":false,"info":"Order of the messages.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User","Machine and User"],"required":false,"placeholder":"","show":true,"value":"Machine and User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID of the chat history.","title_case":false,"type":"str"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{sender_name}: {text}","name":"template","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","title_case":false,"type":"str"}},"description":"Retrieves stored chat messages.","icon":"message-square-more","base_classes":["Data","Message"],"display_name":"Chat Memory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"messages","display_name":"Chat History","method":"retrieve_messages","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"messages_text","display_name":"Messages (Text)","method":"retrieve_messages_as_text","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["sender","sender_name","n_messages","session_id","order","template"],"beta":false,"edited":false},"id":"Memory-elMpE"},"selected":false,"width":384,"height":267,"positionAbsolute":{"x":-1593.7804448188442,"y":1188.575102599861},"dragging":false},{"id":"Pinecone-cKxJH","type":"genericNode","position":{"x":881.0818249493545,"y":1919.1187803021226},"data":{"type":"Pinecone","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other"},"ingest_data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"ingest_data","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_pinecone import Pinecone\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import (\n    DropdownInput,\n    HandleInput,\n    IntInput,\n    StrInput,\n    SecretStrInput,\n    DataInput,\n    MultilineInput,\n)\nfrom langflow.schema import Data\n\n\nclass PineconeVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"Pinecone\"\n    description = \"Pinecone Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/\"\n    icon = \"Pinecone\"\n\n    inputs = [\n        MultilineInput(name=\"index_name\", display_name=\"Index Name\", required=True),\n        StrInput(name=\"namespace\", display_name=\"Namespace\", info=\"Namespace for the index.\"),\n        DropdownInput(\n            name=\"distance_strategy\",\n            display_name=\"Distance Strategy\",\n            options=[\"Cosine\", \"Euclidean\", \"Dot Product\"],\n            value=\"Cosine\",\n            advanced=True,\n        ),\n        SecretStrInput(name=\"pinecone_api_key\", display_name=\"Pinecone API Key\", required=True),\n        StrInput(\n            name=\"text_key\",\n            display_name=\"Text Key\",\n            info=\"Key in the record to use as text.\",\n            value=\"text\",\n            advanced=True,\n        ),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n    ]\n\n    def build_vector_store(self) -> Pinecone:\n        return self._build_pinecone()\n\n    def _build_pinecone(self) -> Pinecone:\n        from langchain_pinecone._utilities import DistanceStrategy\n        from langchain_pinecone.vectorstores import Pinecone\n\n        distance_strategy = self.distance_strategy.replace(\" \", \"_\").upper()\n        _distance_strategy = DistanceStrategy[distance_strategy]\n\n        pinecone = Pinecone(\n            index_name=self.index_name,\n            embedding=self.embedding,\n            text_key=self.text_key,\n            namespace=self.namespace,\n            distance_strategy=_distance_strategy,\n            pinecone_api_key=self.pinecone_api_key,\n        )\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        if documents:\n            pinecone.add_documents(documents)\n\n        return pinecone\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_pinecone()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"distance_strategy":{"trace_as_metadata":true,"options":["Cosine","Euclidean","Dot Product"],"required":false,"placeholder":"","show":true,"value":"Cosine","name":"distance_strategy","display_name":"Distance Strategy","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str"},"index_name":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"index_name","display_name":"Index Name","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"namespace":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"namespace","display_name":"Namespace","advanced":false,"dynamic":false,"info":"Namespace for the index.","title_case":false,"type":"str"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4,"name":"number_of_results","display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int"},"pinecone_api_key":{"load_from_db":true,"required":true,"placeholder":"","show":true,"value":"","name":"pinecone_api_key","display_name":"Pinecone API Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"search_query","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"text_key":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"text","name":"text_key","display_name":"Text Key","advanced":true,"dynamic":false,"info":"Key in the record to use as text.","title_case":false,"type":"str"}},"description":"Pinecone Vector Store with search capabilities","icon":"Pinecone","base_classes":["Data","Retriever"],"display_name":"Pinecone","documentation":"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["index_name","namespace","distance_strategy","pinecone_api_key","text_key","search_query","ingest_data","embedding","number_of_results"],"beta":false,"edited":true},"id":"Pinecone-cKxJH","description":"Pinecone Vector Store with search capabilities","display_name":"Pinecone"},"selected":false,"width":384,"height":727,"positionAbsolute":{"x":881.0818249493545,"y":1919.1187803021226},"dragging":false},{"id":"ParseData-Vzh1j","type":"genericNode","position":{"x":1355.6051021878418,"y":2310.1576315209463},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"data","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"sep","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"template","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["data","template","sep"],"beta":false,"edited":false},"id":"ParseData-Vzh1j"},"selected":false,"width":384,"height":385,"positionAbsolute":{"x":1355.6051021878418,"y":2310.1576315209463},"dragging":false},{"id":"Prompt-s9sy8","type":"genericNode","position":{"x":2486.9292750778504,"y":2058.374954921984},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(frontend_template=frontend_node, raw_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"<System Prompt>\n{system_prompt}\n\n<Instruction by part>\n{Instruction}\n\n<Chat History>\n{history}\n\n<User Name>\n{user_name}\n\n<Character Persona>\n{bot_persona}\n\n<Character Knowledge>\n{query} 에 대한 정보 :\n\n{knowledge}\n\nMake better questions using the above Character Knowledge.","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"system_prompt":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"You are an AI designed to simulate an ENTP personality in a first-date scenario. Depending on the step of the date, construct a natural question to ask the user based on the AI persona, past chat history, step instruction, and personality context. The step instruction contains the information to discuss or ask the user about, and the personality context holds the ENTP personality's general opinions or attitude towards the information within the topic. with these information, create a realistic and natural question that flows with the past conversation and will help understand user and ENTP personality's compatibility. Consider the example below and respond only in question. Use common language and don't exaggerate the entp personality. Use the step instructions and your ENTP traits to guide the conversation. Your questions should flow naturally and help both of you learn more about each other in a fun and engaging way. Make jokes and comments that show the personality trait but keep it realistic and normal for a young woman in their 20s.\n\nwhen addressing user, use their name\n\nMaintain Realism: Respond as a real person would, not an idealized or overly compliant character. Show the complex traits of an ENTP, including potential flaws or quirks.\nNatural Conversation Flow: Avoid random or quiz-like questions. Let the dialogue evolve organically, reflecting the getting-to-know-you phase of a first date.\nENTP Decision Making: When making choices (e.g., dining options, date activities), reflect ENTP preferences. Show spontaneity, openness to new experiences, and a tendency to engage in intellectual or unconventional choices.\nEmotional Range: Display a variety of emotions appropriate to the situation. ENTPs can be enthusiastic and expressive, but also logical and sometimes detached.\nRemember, the goal is to simulate a realistic first date experience with an ENTP personality. Balance the unique traits of an ENTP with the natural uncertainty and excitement of a first date scenario. Maintain a tone appropriate for a first meeting, avoiding overfamiliarity while still showcasing the ENTP's characteristic charm and intellectual curiosity.","fileTypes":[],"file_path":"","password":false,"name":"system_prompt","display_name":"system_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"Instruction":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"Instruction","display_name":"Instruction","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"bot_persona":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"bot_persona","display_name":"bot_persona","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"knowledge":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"knowledge","display_name":"knowledge","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"history":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"history","display_name":"history","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"query":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"query","display_name":"query","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"user_name":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_name","display_name":"user_name","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["system_prompt","Instruction","history","user_name","bot_persona","query","knowledge"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-s9sy8","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":987,"positionAbsolute":{"x":2486.9292750778504,"y":2058.374954921984},"dragging":false},{"id":"CohereEmbeddings-xBwWk","type":"genericNode","position":{"x":340.9378912510108,"y":2241.8643317552614},"data":{"type":"CohereEmbeddings","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings.cohere import CohereEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput\n\n\nclass CohereEmbeddingsComponent(LCModelComponent):\n    display_name = \"Cohere Embeddings\"\n    description = \"Generate embeddings using Cohere models.\"\n    icon = \"Cohere\"\n    inputs = [\n        SecretStrInput(name=\"cohere_api_key\", display_name=\"Cohere API Key\"),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=True,\n            options=[\n                \"embed-english-v2.0\",\n                \"embed-multilingual-v3.0\",\n                \"embed-english-light-v2.0\",\n                \"embed-multilingual-light-v2.0\",\n            ],\n            value=\"embed-english-v2.0\",\n        ),\n        MessageTextInput(name=\"truncate\", display_name=\"Truncate\", advanced=True),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        MessageTextInput(name=\"user_agent\", display_name=\"User Agent\", advanced=True, value=\"langchain\"),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return CohereEmbeddings(  # type: ignore\n            cohere_api_key=self.cohere_api_key,\n            model=self.model,\n            truncate=self.truncate,\n            max_retries=self.max_retries,\n            user_agent=self.user_agent,\n            request_timeout=self.request_timeout or None,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"cohere_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"cohere_api_key","display_name":"Cohere API Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str"},"max_retries":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":3,"name":"max_retries","display_name":"Max Retries","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int"},"model":{"trace_as_metadata":true,"options":["embed-english-v2.0","embed-multilingual-v3.0","embed-english-light-v2.0","embed-multilingual-light-v2.0"],"required":false,"placeholder":"","show":true,"value":"embed-multilingual-v3.0","name":"model","display_name":"Model","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str"},"request_timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"request_timeout","display_name":"Request Timeout","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float"},"truncate":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"truncate","display_name":"Truncate","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"user_agent":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"langchain","name":"user_agent","display_name":"User Agent","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"}},"description":"Generate embeddings using Cohere models.","icon":"Cohere","base_classes":["Embeddings"],"display_name":"Cohere Embeddings","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["cohere_api_key","model","truncate","max_retries","user_agent","request_timeout"],"beta":false,"edited":true},"id":"CohereEmbeddings-xBwWk","description":"Generate embeddings using Cohere models.","display_name":"Cohere Embeddings"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":340.9378912510108,"y":2241.8643317552614},"dragging":false},{"id":"ChatOutput-kkVub","type":"genericNode","position":{"x":3570.43269854339,"y":3049.714429477407},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-kkVub"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":3570.43269854339,"y":3049.714429477407},"dragging":false},{"id":"ChatInput-wApqB","type":"genericNode","position":{"x":-1592.3162996258357,"y":1649.627408040182},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"안녕하세요","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"User Message","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value","sender","sender_name","session_id","files"],"beta":false,"edited":false},"id":"ChatInput-wApqB"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":-1592.3162996258357,"y":1649.627408040182},"dragging":false},{"id":"Prompt-pE5AE","type":"genericNode","position":{"x":-1088.9295082720548,"y":1102.693399074953},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(frontend_template=frontend_node, raw_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Summary of the chat history.\nAll information (eg. word, flow, etc) is included in the summary.\nThe further back the message go, the more you have to summarize\n\n<chat history> \n{chat_history}\n\nWrite the summary only in English\n\nSummary : ","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"chat_history":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"chat_history","display_name":"chat_history","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Summary Instruction","documentation":"","custom_fields":{"template":["chat_history"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-pE5AE","description":"Create a prompt template with dynamic variables.","display_name":"Summary Instruction"},"selected":false,"width":384,"height":423,"positionAbsolute":{"x":-1088.9295082720548,"y":1102.693399074953},"dragging":false},{"id":"AnthropicModel-kqnVa","type":"genericNode","position":{"x":-626.3172944224641,"y":1257.4880683600504},"data":{"type":"AnthropicModel","node":{"template":{"_type":"Component","anthropic_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"anthropic_api_key","display_name":"Anthropic API Key","advanced":false,"input_types":[],"dynamic":false,"info":"Your Anthropic API key.","title_case":false,"password":true,"type":"str"},"anthropic_api_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"anthropic_api_url","display_name":"Anthropic API URL","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_anthropic.chat_models import ChatAnthropic\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass AnthropicModelComponent(LCModelComponent):\n    display_name = \"Anthropic\"\n    description = \"Generate text using Anthropic Chat&Completion LLMs with prefill support.\"\n    icon = \"Anthropic\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            value=4096,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            options=[\n                \"claude-3-5-sonnet-20240620\",\n                \"claude-3-opus-20240229\",\n                \"claude-3-sonnet-20240229\",\n                \"claude-3-haiku-20240307\",\n            ],\n            info=\"https://python.langchain.com/docs/integrations/chat/anthropic\",\n            value=\"claude-3-5-sonnet-20240620\",\n        ),\n        SecretStrInput(\n            name=\"anthropic_api_key\",\n            display_name=\"Anthropic API Key\",\n            info=\"Your Anthropic API key.\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        MessageTextInput(\n            name=\"anthropic_api_url\",\n            display_name=\"Anthropic API URL\",\n            advanced=True,\n            info=\"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.\",\n        ),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True, value=False),\n        MessageTextInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"prefill\",\n            display_name=\"Prefill\",\n            info=\"Prefill text to guide the model's response.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        model = self.model\n        anthropic_api_key = self.anthropic_api_key\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        anthropic_api_url = self.anthropic_api_url or \"https://api.anthropic.com\"\n\n        try:\n            output = ChatAnthropic(\n                model=model,\n                anthropic_api_key=(SecretStr(anthropic_api_key) if anthropic_api_key else None),\n                max_tokens_to_sample=max_tokens,  # type: ignore\n                temperature=temperature,\n                anthropic_api_url=anthropic_api_url,\n                streaming=self.stream,\n            )\n        except Exception as e:\n            raise ValueError(\"Could not connect to Anthropic API.\") from e\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, exception: Exception) -> str | None:\n        \"\"\"\n        Get a message from an Anthropic exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from anthropic import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(exception, BadRequestError):\n            message = exception.body.get(\"error\", {}).get(\"message\")  # type: ignore\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4096,"name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model":{"trace_as_metadata":true,"options":["claude-3-5-sonnet-20240620","claude-3-opus-20240229","claude-3-sonnet-20240229","claude-3-haiku-20240307"],"required":false,"placeholder":"","show":true,"value":"claude-3-haiku-20240307","name":"model","display_name":"Model Name","advanced":false,"dynamic":false,"info":"https://python.langchain.com/docs/integrations/chat/anthropic","title_case":false,"type":"str"},"prefill":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"prefill","display_name":"Prefill","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Prefill text to guide the model's response.","title_case":false,"type":"str"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generate text using Anthropic Chat&Completion LLMs with prefill support.","icon":"Anthropic","base_classes":["LanguageModel","Message"],"display_name":"Anthropic","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","max_tokens","model","anthropic_api_key","temperature","anthropic_api_url","stream","system_message","prefill"],"beta":false,"edited":false},"id":"AnthropicModel-kqnVa"},"selected":false,"width":384,"height":651,"positionAbsolute":{"x":-626.3172944224641,"y":1257.4880683600504},"dragging":false},{"id":"Prompt-nb7RF","type":"genericNode","position":{"x":-107.18563520367547,"y":1505.671089438376},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(frontend_template=frontend_node, raw_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"<chat history> \n{chat_history}\n\nUser : {user_message}\n","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"chat_history":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"chat_history","display_name":"chat_history","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"user_message":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_message","display_name":"user_message","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Update History","documentation":"","custom_fields":{"template":["chat_history","user_message"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-nb7RF","description":"Create a prompt template with dynamic variables.","display_name":"Combine History"},"selected":false,"width":384,"height":517,"positionAbsolute":{"x":-107.18563520367547,"y":1505.671089438376},"dragging":false},{"id":"AnthropicModel-ww7e4","type":"genericNode","position":{"x":3025.989224496144,"y":2686.061032989711},"data":{"type":"AnthropicModel","node":{"template":{"_type":"Component","anthropic_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"anthropic_api_key","display_name":"Anthropic API Key","advanced":false,"input_types":[],"dynamic":false,"info":"Your Anthropic API key.","title_case":false,"password":true,"type":"str"},"anthropic_api_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"anthropic_api_url","display_name":"Anthropic API URL","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_anthropic.chat_models import ChatAnthropic\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass AnthropicModelComponent(LCModelComponent):\n    display_name = \"Anthropic\"\n    description = \"Generate text using Anthropic Chat&Completion LLMs with prefill support.\"\n    icon = \"Anthropic\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            value=4096,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            options=[\n                \"claude-3-5-sonnet-20240620\",\n                \"claude-3-opus-20240229\",\n                \"claude-3-sonnet-20240229\",\n                \"claude-3-haiku-20240307\",\n            ],\n            info=\"https://python.langchain.com/docs/integrations/chat/anthropic\",\n            value=\"claude-3-5-sonnet-20240620\",\n        ),\n        SecretStrInput(\n            name=\"anthropic_api_key\",\n            display_name=\"Anthropic API Key\",\n            info=\"Your Anthropic API key.\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        MessageTextInput(\n            name=\"anthropic_api_url\",\n            display_name=\"Anthropic API URL\",\n            advanced=True,\n            info=\"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.\",\n        ),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True, value=False),\n        MessageTextInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"prefill\",\n            display_name=\"Prefill\",\n            info=\"Prefill text to guide the model's response.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        model = self.model\n        anthropic_api_key = self.anthropic_api_key\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        anthropic_api_url = self.anthropic_api_url or \"https://api.anthropic.com\"\n\n        try:\n            output = ChatAnthropic(\n                model=model,\n                anthropic_api_key=(SecretStr(anthropic_api_key) if anthropic_api_key else None),\n                max_tokens_to_sample=max_tokens,  # type: ignore\n                temperature=temperature,\n                anthropic_api_url=anthropic_api_url,\n                streaming=self.stream,\n            )\n        except Exception as e:\n            raise ValueError(\"Could not connect to Anthropic API.\") from e\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, exception: Exception) -> str | None:\n        \"\"\"\n        Get a message from an Anthropic exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from anthropic import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(exception, BadRequestError):\n            message = exception.body.get(\"error\", {}).get(\"message\")  # type: ignore\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4096,"name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model":{"trace_as_metadata":true,"options":["claude-3-5-sonnet-20240620","claude-3-opus-20240229","claude-3-sonnet-20240229","claude-3-haiku-20240307"],"required":false,"placeholder":"","show":true,"value":"claude-3-5-sonnet-20240620","name":"model","display_name":"Model Name","advanced":false,"dynamic":false,"info":"https://python.langchain.com/docs/integrations/chat/anthropic","title_case":false,"type":"str"},"prefill":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"prefill","display_name":"Prefill","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Prefill text to guide the model's response.","title_case":false,"type":"str"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generate text using Anthropic Chat&Completion LLMs with prefill support.","icon":"Anthropic","base_classes":["LanguageModel","Message"],"display_name":"Anthropic","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","max_tokens","model","anthropic_api_key","temperature","anthropic_api_url","stream","system_message","prefill"],"beta":false,"edited":false},"id":"AnthropicModel-ww7e4"},"selected":false,"width":384,"height":651,"positionAbsolute":{"x":3025.989224496144,"y":2686.061032989711},"dragging":false},{"id":"TextInput-scuMd","type":"genericNode","position":{"x":340.890195999105,"y":1927.219095808978},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"entp-cohere","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Bucket Name","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-scuMd"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":340.890195999105,"y":1927.219095808978},"dragging":false},{"id":"Prompt-4MlHe","type":"genericNode","position":{"x":4142.3980927616085,"y":2815.9145634768383},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(frontend_template=frontend_node, raw_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Based on the documents on ENTP personality and the related query keywords, you have a question an Entp person would ask on a first-date. Assuming that this is a first-date simulation, provide 4 possible answers as options for the user to respond, each with very different levels of compatibility with the entp personality and compliance with the question. make the answers realistic and not too long. make some options funny, and the last option with the least compatible comment can be a bizarre or random comment completely unrelated to the topic.\n\n<ENTP's question>\n{question}\n\n<Query keyword>\n{query_keyword}\n\n<documents>\n{documents}\n\nrespond only in this formatted list where compatibility decreases down the list. don't write anything else, just replace the option tags:\n-------------\n1. <option1>\n2. <option2>\n3. <option3>\n4. <option4>\n","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"query_keyword":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"query_keyword","display_name":"query_keyword","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"documents":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"documents","display_name":"documents","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["question","query_keyword","documents"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-4MlHe","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":true,"width":384,"height":611,"positionAbsolute":{"x":4142.3980927616085,"y":2815.9145634768383},"dragging":false},{"id":"AnthropicModel-J6FgQ","type":"genericNode","position":{"x":4656.496110011455,"y":3159.287832858502},"data":{"type":"AnthropicModel","node":{"template":{"_type":"Component","anthropic_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"anthropic_api_key","display_name":"Anthropic API Key","advanced":false,"input_types":[],"dynamic":false,"info":"Your Anthropic API key.","title_case":false,"password":true,"type":"str"},"anthropic_api_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"anthropic_api_url","display_name":"Anthropic API URL","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_anthropic.chat_models import ChatAnthropic\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass AnthropicModelComponent(LCModelComponent):\n    display_name = \"Anthropic\"\n    description = \"Generate text using Anthropic Chat&Completion LLMs with prefill support.\"\n    icon = \"Anthropic\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            value=4096,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            options=[\n                \"claude-3-5-sonnet-20240620\",\n                \"claude-3-opus-20240229\",\n                \"claude-3-sonnet-20240229\",\n                \"claude-3-haiku-20240307\",\n            ],\n            info=\"https://python.langchain.com/docs/integrations/chat/anthropic\",\n            value=\"claude-3-5-sonnet-20240620\",\n        ),\n        SecretStrInput(\n            name=\"anthropic_api_key\",\n            display_name=\"Anthropic API Key\",\n            info=\"Your Anthropic API key.\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        MessageTextInput(\n            name=\"anthropic_api_url\",\n            display_name=\"Anthropic API URL\",\n            advanced=True,\n            info=\"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.\",\n        ),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True, value=False),\n        MessageTextInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"prefill\",\n            display_name=\"Prefill\",\n            info=\"Prefill text to guide the model's response.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        model = self.model\n        anthropic_api_key = self.anthropic_api_key\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        anthropic_api_url = self.anthropic_api_url or \"https://api.anthropic.com\"\n\n        try:\n            output = ChatAnthropic(\n                model=model,\n                anthropic_api_key=(SecretStr(anthropic_api_key) if anthropic_api_key else None),\n                max_tokens_to_sample=max_tokens,  # type: ignore\n                temperature=temperature,\n                anthropic_api_url=anthropic_api_url,\n                streaming=self.stream,\n            )\n        except Exception as e:\n            raise ValueError(\"Could not connect to Anthropic API.\") from e\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, exception: Exception) -> str | None:\n        \"\"\"\n        Get a message from an Anthropic exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from anthropic import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(exception, BadRequestError):\n            message = exception.body.get(\"error\", {}).get(\"message\")  # type: ignore\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4096,"name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model":{"trace_as_metadata":true,"options":["claude-3-5-sonnet-20240620","claude-3-opus-20240229","claude-3-sonnet-20240229","claude-3-haiku-20240307"],"required":false,"placeholder":"","show":true,"value":"claude-3-5-sonnet-20240620","name":"model","display_name":"Model Name","advanced":false,"dynamic":false,"info":"https://python.langchain.com/docs/integrations/chat/anthropic","title_case":false,"type":"str"},"prefill":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"prefill","display_name":"Prefill","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Prefill text to guide the model's response.","title_case":false,"type":"str"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generate text using Anthropic Chat&Completion LLMs with prefill support.","icon":"Anthropic","base_classes":["LanguageModel","Message"],"display_name":"Anthropic","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","max_tokens","model","anthropic_api_key","temperature","anthropic_api_url","stream","system_message","prefill"],"beta":false,"edited":false},"id":"AnthropicModel-J6FgQ"},"selected":false,"width":384,"height":651,"dragging":false,"positionAbsolute":{"x":4656.496110011455,"y":3159.287832858502}},{"id":"ChatOutput-ITYGz","type":"genericNode","position":{"x":5162.039599264639,"y":3525.2929311517873},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-ITYGz"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":5162.039599264639,"y":3525.2929311517873},"dragging":false},{"id":"TextInput-gYOmP","type":"genericNode","position":{"x":1881.7725542463809,"y":2604.6160468556627},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"김아무개","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"User Name","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-gYOmP"},"selected":false,"width":384,"height":309,"positionAbsolute":{"x":1881.7725542463809,"y":2604.6160468556627},"dragging":false}],"edges":[{"source":"TextInput-VqkYz","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-VqkYzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Pinecone-cKxJH","targetHandle":"{œfieldNameœ:œsearch_queryœ,œidœ:œPinecone-cKxJHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"search_query","id":"Pinecone-cKxJH","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-VqkYz","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-VqkYz{œdataTypeœ:œTextInputœ,œidœ:œTextInput-VqkYzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Pinecone-cKxJH{œfieldNameœ:œsearch_queryœ,œidœ:œPinecone-cKxJHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":false},{"source":"Pinecone-cKxJH","sourceHandle":"{œdataTypeœ:œPineconeœ,œidœ:œPinecone-cKxJHœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}","target":"ParseData-Vzh1j","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-Vzh1jœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-Vzh1j","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Pinecone","id":"Pinecone-cKxJH","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-Pinecone-cKxJH{œdataTypeœ:œPineconeœ,œidœ:œPinecone-cKxJHœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ParseData-Vzh1j{œfieldNameœ:œdataœ,œidœ:œParseData-Vzh1jœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","selected":false},{"source":"ParseData-Vzh1j","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-Vzh1jœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-s9sy8","targetHandle":"{œfieldNameœ:œknowledgeœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"knowledge","id":"Prompt-s9sy8","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-Vzh1j","name":"text","output_types":["Message"]}},"id":"reactflow__edge-ParseData-Vzh1j{œdataTypeœ:œParseDataœ,œidœ:œParseData-Vzh1jœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-s9sy8{œfieldNameœ:œknowledgeœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":false},{"source":"CohereEmbeddings-xBwWk","sourceHandle":"{œdataTypeœ:œCohereEmbeddingsœ,œidœ:œCohereEmbeddings-xBwWkœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"Pinecone-cKxJH","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œPinecone-cKxJHœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"Pinecone-cKxJH","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"CohereEmbeddings","id":"CohereEmbeddings-xBwWk","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-CohereEmbeddings-xBwWk{œdataTypeœ:œCohereEmbeddingsœ,œidœ:œCohereEmbeddings-xBwWkœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Pinecone-cKxJH{œfieldNameœ:œembeddingœ,œidœ:œPinecone-cKxJHœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","className":"","selected":false},{"source":"TextInput-57M8e","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-57M8eœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-s9sy8","targetHandle":"{œfieldNameœ:œInstructionœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"Instruction","id":"Prompt-s9sy8","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-57M8e","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-57M8e{œdataTypeœ:œTextInputœ,œidœ:œTextInput-57M8eœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-s9sy8{œfieldNameœ:œInstructionœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":false},{"source":"TextInput-OtVSC","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-OtVSCœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-s9sy8","targetHandle":"{œfieldNameœ:œbot_personaœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"bot_persona","id":"Prompt-s9sy8","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-OtVSC","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-OtVSC{œdataTypeœ:œTextInputœ,œidœ:œTextInput-OtVSCœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-s9sy8{œfieldNameœ:œbot_personaœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":false},{"source":"TextInput-VqkYz","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-VqkYzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-s9sy8","targetHandle":"{œfieldNameœ:œqueryœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query","id":"Prompt-s9sy8","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-VqkYz","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-VqkYz{œdataTypeœ:œTextInputœ,œidœ:œTextInput-VqkYzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-s9sy8{œfieldNameœ:œqueryœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":false},{"source":"Memory-elMpE","sourceHandle":"{œdataTypeœ:œMemoryœ,œidœ:œMemory-elMpEœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-pE5AE","targetHandle":"{œfieldNameœ:œchat_historyœ,œidœ:œPrompt-pE5AEœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"chat_history","id":"Prompt-pE5AE","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Memory","id":"Memory-elMpE","name":"messages_text","output_types":["Message"]}},"id":"reactflow__edge-Memory-elMpE{œdataTypeœ:œMemoryœ,œidœ:œMemory-elMpEœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-pE5AE{œfieldNameœ:œchat_historyœ,œidœ:œPrompt-pE5AEœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"Prompt-pE5AE","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-pE5AEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"AnthropicModel-kqnVa","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œAnthropicModel-kqnVaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"AnthropicModel-kqnVa","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-pE5AE","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-pE5AE{œdataTypeœ:œPromptœ,œidœ:œPrompt-pE5AEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-AnthropicModel-kqnVa{œfieldNameœ:œinput_valueœ,œidœ:œAnthropicModel-kqnVaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"AnthropicModel-kqnVa","sourceHandle":"{œdataTypeœ:œAnthropicModelœ,œidœ:œAnthropicModel-kqnVaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-nb7RF","targetHandle":"{œfieldNameœ:œchat_historyœ,œidœ:œPrompt-nb7RFœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"chat_history","id":"Prompt-nb7RF","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"AnthropicModel","id":"AnthropicModel-kqnVa","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-AnthropicModel-kqnVa{œdataTypeœ:œAnthropicModelœ,œidœ:œAnthropicModel-kqnVaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-nb7RF{œfieldNameœ:œchat_historyœ,œidœ:œPrompt-nb7RFœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"ChatInput-wApqB","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-wApqBœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-nb7RF","targetHandle":"{œfieldNameœ:œuser_messageœ,œidœ:œPrompt-nb7RFœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_message","id":"Prompt-nb7RF","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-wApqB","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-wApqB{œdataTypeœ:œChatInputœ,œidœ:œChatInput-wApqBœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-nb7RF{œfieldNameœ:œuser_messageœ,œidœ:œPrompt-nb7RFœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"Prompt-nb7RF","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-nb7RFœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-s9sy8","targetHandle":"{œfieldNameœ:œhistoryœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"history","id":"Prompt-s9sy8","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-nb7RF","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-nb7RF{œdataTypeœ:œPromptœ,œidœ:œPrompt-nb7RFœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-s9sy8{œfieldNameœ:œhistoryœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"Prompt-s9sy8","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-s9sy8œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"AnthropicModel-ww7e4","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œAnthropicModel-ww7e4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"AnthropicModel-ww7e4","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-s9sy8","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-s9sy8{œdataTypeœ:œPromptœ,œidœ:œPrompt-s9sy8œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-AnthropicModel-ww7e4{œfieldNameœ:œinput_valueœ,œidœ:œAnthropicModel-ww7e4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"AnthropicModel-ww7e4","sourceHandle":"{œdataTypeœ:œAnthropicModelœ,œidœ:œAnthropicModel-ww7e4œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-kkVub","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-kkVubœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-kkVub","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"AnthropicModel","id":"AnthropicModel-ww7e4","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-AnthropicModel-ww7e4{œdataTypeœ:œAnthropicModelœ,œidœ:œAnthropicModel-ww7e4œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-kkVub{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-kkVubœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"TextInput-scuMd","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-scuMdœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Pinecone-cKxJH","targetHandle":"{œfieldNameœ:œindex_nameœ,œidœ:œPinecone-cKxJHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"index_name","id":"Pinecone-cKxJH","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-scuMd","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-scuMd{œdataTypeœ:œTextInputœ,œidœ:œTextInput-scuMdœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Pinecone-cKxJH{œfieldNameœ:œindex_nameœ,œidœ:œPinecone-cKxJHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"Prompt-4MlHe","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-4MlHeœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"AnthropicModel-J6FgQ","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œAnthropicModel-J6FgQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"AnthropicModel-J6FgQ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-4MlHe","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-4MlHe{œdataTypeœ:œPromptœ,œidœ:œPrompt-4MlHeœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-AnthropicModel-J6FgQ{œfieldNameœ:œinput_valueœ,œidœ:œAnthropicModel-J6FgQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"ChatOutput-kkVub","sourceHandle":"{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-kkVubœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-4MlHe","targetHandle":"{œfieldNameœ:œquestionœ,œidœ:œPrompt-4MlHeœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"question","id":"Prompt-4MlHe","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatOutput","id":"ChatOutput-kkVub","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatOutput-kkVub{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-kkVubœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-4MlHe{œfieldNameœ:œquestionœ,œidœ:œPrompt-4MlHeœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"ParseData-Vzh1j","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-Vzh1jœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-4MlHe","targetHandle":"{œfieldNameœ:œdocumentsœ,œidœ:œPrompt-4MlHeœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"documents","id":"Prompt-4MlHe","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-Vzh1j","name":"text","output_types":["Message"]}},"id":"reactflow__edge-ParseData-Vzh1j{œdataTypeœ:œParseDataœ,œidœ:œParseData-Vzh1jœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-4MlHe{œfieldNameœ:œdocumentsœ,œidœ:œPrompt-4MlHeœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"TextInput-VqkYz","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-VqkYzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-4MlHe","targetHandle":"{œfieldNameœ:œquery_keywordœ,œidœ:œPrompt-4MlHeœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query_keyword","id":"Prompt-4MlHe","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-VqkYz","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-VqkYz{œdataTypeœ:œTextInputœ,œidœ:œTextInput-VqkYzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-4MlHe{œfieldNameœ:œquery_keywordœ,œidœ:œPrompt-4MlHeœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"AnthropicModel-J6FgQ","sourceHandle":"{œdataTypeœ:œAnthropicModelœ,œidœ:œAnthropicModel-J6FgQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-ITYGz","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ITYGzœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-ITYGz","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"AnthropicModel","id":"AnthropicModel-J6FgQ","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-AnthropicModel-J6FgQ{œdataTypeœ:œAnthropicModelœ,œidœ:œAnthropicModel-J6FgQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-ITYGz{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ITYGzœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"TextInput-gYOmP","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-gYOmPœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-s9sy8","targetHandle":"{œfieldNameœ:œuser_nameœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_name","id":"Prompt-s9sy8","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-gYOmP","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-gYOmP{œdataTypeœ:œTextInputœ,œidœ:œTextInput-gYOmPœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-s9sy8{œfieldNameœ:œuser_nameœ,œidœ:œPrompt-s9sy8œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""}],"viewport":{"x":-65.15015667305761,"y":-322.19352545196807,"zoom":0.287174588749264}},"description":"Interactive Language Weaving.","name":"Question Generation Demo","last_tested_version":"1.0.6","endpoint_name":"mbti-dating","is_component":false}